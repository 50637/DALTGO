{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import pickle\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional\n",
    "from random import shuffle\n",
    "import time\n",
    "# add encode path\n",
    "sys.path.insert(0, './Encode/')\n",
    "import attention_layer\n",
    "import ffn_layer\n",
    "import embedding_layer\n",
    "import model_utils\n",
    "import transformer_encode\n",
    "import hparam\n",
    "import metric\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "flags = tf.compat.v1.flags\n",
    "logging = tf.compat.v1.logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Process sequence length\n",
    "def cut(a, maxlen):\n",
    "   b=[]\n",
    "   for i in a:\n",
    "      if len(i)>maxlen:\n",
    "          start = np.random.randint(0, len(i)-maxlen+1)\n",
    "          b.append(i[start: start+maxlen])\n",
    "      else:\n",
    "          b.append(i)\n",
    "   return b\n",
    "\n",
    "### 'N': 12 means N is the 12th position in this vector table\n",
    "index={'pad': 0,  'N': 12, 'D': 9, 'H': 17, 'K': 7, 'I': 6, 'W': 19, 'P': 11, 'Z': 23, 'M': 16, 'Q': 13, 'V': 3, 'X': 20, 'R': 8, 'S': 5, 'T': 10, 'O': 24, 'Y': 15, 'B': 22, 'F': 14, 'G': 2, 'A': 1, 'E': 4, 'L': 0, 'U': 21, 'C': 18}\n",
    "def to_int(seq, hparam):\n",
    "\tvec=[]\n",
    "\tfor i in seq:\n",
    "\t\tvec.append(index[i])\n",
    "\tfor i in range(len(vec), hparam['MAXLEN']):\n",
    "\t\tvec.append(0)\t\n",
    "\treturn np.array(vec)\n",
    "def go_protein_similarity(seq_embed, hparams):\n",
    "\t#Characteristics of GO terms read from a file\n",
    "\tsparse_matrix = np.load('ccCAFA3weight80.npy')\n",
    "\tsparse_matrix *= hparams['hidden_size'] ** 0.5\n",
    "\tembeddings = tf.constant(np.array(sparse_matrix))\n",
    "\tembeddings = tf.cast(embeddings, tf.float32)\n",
    "\tlabel_embed = tf.expand_dims(embeddings, axis=0)\n",
    "\tlabel_embed = tf.tile(label_embed, [hparams['batch_size'],1,1])\n",
    "\tsimilarity_matrix = tf.matmul(seq_embed, label_embed, transpose_b=True)\n",
    "\tsimilarity_matrix = tf.nn.softmax(similarity_matrix, axis=-1)\n",
    "\tw  = Conv1D(filters=hparams['joint_similarity_filter_size'],kernel_size=hparams['joint_similarity_kernel_size'],\\\n",
    "\t strides=1, padding='same', activation='relu')(similarity_matrix)\n",
    "\tw1 = tf.math.reduce_max(w, axis=-1)\n",
    "\tprint(similarity_matrix)\n",
    "\tw1 = tf.nn.softmax(w1, axis=-1)\n",
    "\tw1 = tf.expand_dims(w1, axis=1)\n",
    "\tw2 = tf.matmul(w1,seq_embed)\n",
    "\treturn tf.squeeze(w2, axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DALTGO_model(object):\n",
    "\n",
    "\tdef __init__(self, hparams):\n",
    "\t\tself.hparams = hparams\t\n",
    "    #Convert the input sequence into a corresponding embedding representation and add position encode\n",
    "\tdef Embedding(self, x):\n",
    "\t\thparams=self.hparams\n",
    "\t\tself.embedding_layer = embedding_layer.EmbeddingSharedWeights(\n",
    "\t\t\t\t\thparams[\"vocab_size\"], hparams[\"hidden_size\"])\n",
    "\t\tembedded_inputs = self.embedding_layer(x)\n",
    "\t\tlstm_layer = Bidirectional(LSTM(units=40, return_sequences=True))\n",
    "\t\twith tf.name_scope(\"add_pos_encoding\"):\n",
    "\t\t\tlength = tf.shape(embedded_inputs)[1]\n",
    "\t\t\tpos_encoding = model_utils.get_position_encoding(\n",
    "\t\t\t\tlength, hparams[\"hidden_size\"])\n",
    "\t\t\tencoder_inputs = embedded_inputs + pos_encoding\n",
    "\t\t\tencoder_inputs = lstm_layer(encoder_inputs)\n",
    "\t\tif self.hparams['train']:\n",
    "\t\t\t\tencoder_inputs = tf.nn.dropout(\n",
    "\t\t\t\t\tencoder_inputs, rate=self.hparams[\"layer_postprocess_dropout\"])\n",
    "\t\tself.inputs_padding = model_utils.get_padding(x)\n",
    "\t\tself.attention_bias = model_utils.get_padding_bias(x)\n",
    "\t\treturn encoder_inputs\n",
    "\n",
    "\t# Transfomer-based sequence encoder\n",
    "\tdef Encoder(self, encoder_inputs):\n",
    "\t\thparams=self.hparams\n",
    "\t\tself.encoder_stack = transformer_encode.EncoderStack(hparams)\n",
    "\t\treturn self.encoder_stack(encoder_inputs, self.attention_bias, self.inputs_padding)\n",
    "\n",
    "\tdef Main_model(self):\n",
    "\t\thparams=self.hparams\n",
    "\t\ttf.compat.v1.disable_eager_execution()\n",
    "\t\tinputs = tf.compat.v1.placeholder(shape=(self.hparams['batch_size'], self.hparams['MAXLEN']), dtype=tf.int32)\n",
    "\t\touts = tf.compat.v1.placeholder(shape=(self.hparams['batch_size'], self.hparams['nb_classes']), dtype=tf.int32)\n",
    "\t\treturn_box = [inputs, outs]\n",
    "\t\tencoder_inputs = self.Embedding(inputs)\n",
    "\t\tencoder_outputs = self.Encoder(encoder_inputs)\n",
    "\t\tdef output_layer(input):\n",
    "\t\t\t# argv the input is a tensor with shape [batch, length, hidden_size]\n",
    "\t\t\tout1 = tf.keras.layers.MaxPool1D(8, data_format='channels_first')(input)\n",
    "\t\t\tout1 = tf.reshape(out1, [hparams['batch_size'], -1])\n",
    "\t\t\tout2 = tf.compat.v1.layers.Dense(hparams['nb_classes'], activation='sigmoid', name='dense_out')(out1)\n",
    "\t\t\treturn out2\n",
    "\n",
    "\t\tif (hparams['label_embed']==False):\n",
    "\t\t\t\t#Corresponds to baseline model M2\n",
    "\t\t\t\tpred_out = output_layer(encoder_outputs)\n",
    "\t\t\t\tloss = self.loss(outs, pred_out, 'bc')\n",
    "\t\t\t\treturn_box.append(loss)\n",
    "\t\t\t\treturn_box.append(pred_out)\n",
    "\t\telse:#Corresponds to baseline model M3\n",
    "\t\t\t\tout1 = go_protein_similarity(encoder_outputs,hparams)\n",
    "\t\t\t\tpred_out=tf.compat.v1.layers.Dense(hparams['nb_classes'], activation='sigmoid', name='dense_out')(out1)\n",
    "\t\t\t\tloss = self.loss(outs, pred_out, 'bc')\n",
    "\t\t\t\treturn_box.append(loss)\n",
    "\t\t\t\treturn_box.append(pred_out)\n",
    "\t\tif len (return_box)<5:\n",
    "\t\t\treturn_box.append(tf.constant([0]))\n",
    "\t\treturn return_box\n",
    "\n",
    "\tdef loss(self, ytrue, ypred, loss_type):\n",
    "\t\tif loss_type == 'bc':\n",
    "\t\t\tbce=tf.keras.losses.BinaryCrossentropy()\n",
    "\t\t\treturn bce(ytrue, ypred)\n",
    "\n",
    "\tdef train(self):\n",
    "\t\thparams=self.hparams\n",
    "\t\tdata = self.data_load(self.hparams['data_path'])\n",
    "\t\tdef sparse_to_dense(y,  length):\n",
    "\t\t\tout=np.zeros((len(y), length), dtype=np.int32)\n",
    "\t\t\tfor i in range(len(y)):\n",
    "\t\t\t\t#print (y[i])\n",
    "\t\t\t\tfor j in y[i]:\n",
    "\t\t\t\t\tout[i][j]=1\n",
    "\t\t\treturn out\n",
    "\t\t\t\n",
    "\t\twith tf.device('/gpu:0'):\n",
    "\t\t\tholder_list = self.Main_model()  #------------holder_list: [model_input, model_output, loss]\n",
    "\t\t\toptimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=self.hparams['lr'])\n",
    "\t\t\ttrain_op = optimizer.minimize(holder_list[2])\n",
    "\t\t\tinit_op = tf.compat.v1.global_variables_initializer()\n",
    "\t\tbatch_size = self.hparams['batch_size']\n",
    "\t\ttrain_x = data[0]\n",
    "\t\ttrain_y  = sparse_to_dense(data[1] ,hparams['nb_classes'])\n",
    "\t\tval_x = data[2]\n",
    "\t\tval_y= sparse_to_dense(data[3], hparams['nb_classes'])\n",
    "\t\tval_list=[v for v in tf.compat.v1.global_variables()]\n",
    "\t\tsaver = tf.compat.v1.train.Saver(val_list, max_to_keep=None)\n",
    "\t\tprint ('start training. training information:')\n",
    "\t\twith tf.compat.v1.Session() as sess:\n",
    "\t\t\tsess.run(init_op)\n",
    "\t\t\tresume_epoch = 0\t\t\t\n",
    "\t\t\tfor epoch in range(hparams['epochs']):\n",
    "\t\t\t\tsepoch_train_loss = 0.\n",
    "\t\t\t\titerations = int((len(train_x)) // hparams['batch_size'])\n",
    "\t\t\t\tprint (\"epoch %d begins:\" %(resume_epoch+epoch+1))\n",
    "\t\t\t\tprint (\"#iterations:\", iterations)\n",
    "\t\t\t\tfor ite in range(iterations):\n",
    "\t\t\t\t\tx = cut(train_x[ite*batch_size: (ite+1)*batch_size], hparams['MAXLEN'])\n",
    "\t\t\t\t\ty = train_y[ite*batch_size: (ite+1)*batch_size]\n",
    "\t\t\t\t\ttrain_loss ,_ , regular_loss= sess.run([holder_list[2], train_op, holder_list[4]], {holder_list[0]: x, holder_list[1]: y})\n",
    "\t\t\t\t\tsepoch_train_loss+=train_loss\n",
    "\t\t\t\t\tprint (\"iteration %d/%d totaltrain_loss: %.3f\" %(ite+1, iterations, train_loss))\n",
    "\t\t\t\tsepoch_train_loss /= iterations\n",
    "\t\t\t\ttrain_z = list(zip(train_x, train_y))\n",
    "\t\t\t\tshuffle(train_z)\n",
    "\t\t\t\ttrain_x, train_y = zip(*train_z)\n",
    "\t\t\t#evaluation\n",
    "\t\t\tepoch = hparams['epochs']-1\n",
    "\t\t\tpred_scores=[]\n",
    "\t\t\tsepoch_val_loss = 0.\n",
    "\t\t\titerations = int(len(val_x) // hparams['batch_size'])\n",
    "\t\t\tfor ite in range(iterations):\n",
    "\t\t\t\tx= val_x[ite*batch_size: (ite+1)*batch_size]\n",
    "\t\t\t\ty= val_y[ite*batch_size: (ite+1)*batch_size]\n",
    "\t\t\t\tval_loss, pred_score = sess.run([holder_list[2], holder_list[3]], {holder_list[0]: x, holder_list[1]: y})\n",
    "\t\t\t\tsepoch_val_loss+=val_loss\n",
    "\t\t\t\tpred_scores.extend(pred_score)\n",
    "\t\t\t\tprint (\"iteration %d/%d val_loss: %.3f\" %(ite+1, iterations, val_loss))\n",
    "\t\t\tsepoch_val_loss/=iterations\n",
    "\t\t\tfmax, smin, auprc,auroc_macro,auroc_micro = metric.main(val_y[:len(pred_scores)], pred_scores, hparams)\n",
    "\t\t\tprint(fmax)\n",
    "\t\t\tprint(smin)\n",
    "\t\t\tprint(auprc)\n",
    "\t\t\tprint(auroc_micro)\n",
    "\t\t\tprint(auroc_macro)\n",
    "\t\t\tprint(\" %.3f %.3f %.3f\\n\" %(fmax, smin, auprc))\t\n",
    "\t\t\twith open(\"result.csv\", \"a\") as f:\n",
    "\t\t\t\tf.write(\"%d %.3f %.3f %.3f %.3f%.3f %.3f\\n\" %(epoch+resume_epoch+1, sepoch_val_loss, fmax, smin, auprc,auroc_macro,auroc_micro))\t\t\n",
    "\t\t\t\t\t\n",
    "\tdef data_load(self, path):\n",
    "\t\twith open(path+\"/train_seq_\"+hparams['ontology'], \"rb\") as f:\n",
    "\t\t\ttrain_seq = pickle.load(f)\n",
    "\t\twith open(path+\"/train_label_\"+hparams['ontology'], \"rb\") as f:\n",
    "\t\t\ttrain_label = pickle.load(f)\n",
    "\t\twith open(path+\"/test_seq_\"+hparams['ontology'], \"rb\") as f:\n",
    "\t\t\ttest_seq = pickle.load(f)\n",
    "\t\twith open(path+\"/test_label_\"+hparams['ontology'], \"rb\") as f:\n",
    "\t\t\ttest_label = pickle.load(f)\n",
    "\t\ttrain_X=[]\n",
    "\t\ttrain_Y= train_label\n",
    "\t\ttest_X=[]\n",
    "\t\ttest_Y= test_label\n",
    "\t\tfor i in range(len(train_seq)):\n",
    "\t\t\ttrain_X.append(to_int(train_seq[i]['seq'], self.hparams))\n",
    "\t\tfor i in range(len(test_seq)):\n",
    "\t\t\ttest_X.append(to_int(test_seq[i]['seq'], self.hparams))\n",
    "\t\t#Select the return data based on the training mode\n",
    "\t\treturn train_X, train_Y, test_X, test_Y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__== \"__main__\":\n",
    "\tflags = tf.compat.v1.flags\n",
    "\tflags.DEFINE_integer(\"batch_size\", 32, \"e\")\n",
    "\tflags.DEFINE_integer(\"epochs\", 30, \"e\")\n",
    "\tflags.DEFINE_float(\"lr\", 1e-3, \"e\")\n",
    "\tflags.DEFINE_string(\"save_path\", './', \"model savepath\")\n",
    "\tflags.DEFINE_string(\"ontology\", 'cc', \"e\")\n",
    "\tflags.DEFINE_integer(\"nb_classes\", None, \"e\")\n",
    "\tflags.DEFINE_bool(\"label_embed\", True, \"e\")\n",
    "\tflags.DEFINE_string(\"data_path\", '../data/CAFA3/', \"path_to_store_data\")\n",
    "\tflags.DEFINE_float(\"regular_lambda\", 0, \"e\")\n",
    "\t#the number of transformer attention head\n",
    "\tflags.DEFINE_integer(\"num_heads\", 2, \"e\")\n",
    "\tflags.DEFINE_integer(\"num_hidden_layers\", 6, \"e\")\n",
    "\t#the dimention of vector\n",
    "\tflags.DEFINE_integer(\"hidden_size\", 128, \"e\")\n",
    "\tFLAGS = flags.FLAGS\n",
    "\thparams = hparam.params(flags)\n",
    "\tmodel1 = DALTGO_model(hparams)\n",
    "\tmodel1.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "protein",
   "language": "python",
   "name": "protein"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
